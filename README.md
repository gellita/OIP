# Краулер для сайта [ilibrary.ru](https://ilibrary.ru)

**Команда:** Литвина Ангелина, Зиганшина Камила

Программа автоматически:

- собирает ссылки на страницы текстов с сайта **ilibrary.ru**
- формирует файл `urls.txt` (список найденных страниц)
- скачивает минимум **100 HTML-страниц**
- сохраняет их **без очистки от HTML-разметки**
- создаёт файл `index.txt` (номер файла → URL)

HTML сохраняется полностью, в исходном виде.

---

## Требования

Для запуска необходимы:

- **Python 3.10** или выше
- установленный **pip**

---

## Установка зависимостей

Перед запуском установите необходимые библиотеки:

```bash
pip install requests beautifulsoup4 lxml
```

---

## Запуск программы

1. Откройте **терминал**.
2. Перейдите в папку проекта:
   ```bash
   cd путь_к_папке_проекта
   ```
3. Запустите скрипт:
   ```bash
   python crawler.py
   ```

После запуска программа автоматически:

- соберёт список ссылок;
- создаст файл `urls.txt`;
- скачает не менее 100 HTML-страниц;
- создаст папку `dump/` и сохранит туда страницы;
- сформирует файл `index.txt`, сопоставляющий локальные файлы с исходными URL.

---
